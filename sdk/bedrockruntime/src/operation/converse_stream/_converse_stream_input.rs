// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
#[allow(missing_docs)] // documentation missing in model
#[non_exhaustive]
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::fmt::Debug)]
pub struct ConverseStreamInput {
    /// <p>The ID for the model.</p>
    /// <p>The <code>modelId</code> to provide depends on the type of model that you use:</p>
    /// <ul>
    /// <li>
    /// <p>If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns">Amazon Bedrock base model IDs (on-demand throughput)</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html">Run inference using a Provisioned Throughput</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a custom model, first purchase Provisioned Throughput for it. Then specify the ARN of the resulting provisioned model. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html">Use a custom model in Amazon Bedrock</a> in the Amazon Bedrock User Guide.</p></li>
    /// </ul>
    pub model_id: ::std::option::Option<::std::string::String>,
    /// <p>The messages that you want to send to the model.</p>
    pub messages: ::std::option::Option<::std::vec::Vec<crate::types::Message>>,
    /// <p>A system prompt to send to the model.</p>
    pub system: ::std::option::Option<::std::vec::Vec<crate::types::SystemContentBlock>>,
    /// <p>Inference parameters to pass to the model. <code>ConverseStream</code> supports a base set of inference parameters. If you need to pass additional parameters that the model supports, use the <code>additionalModelRequestFields</code> request field.</p>
    pub inference_config: ::std::option::Option<crate::types::InferenceConfiguration>,
    /// <p>Configuration information for the tools that the model can use when generating a response.</p><note>
    /// <p>This field is only supported by Anthropic Claude 3 models.</p>
    /// </note>
    pub tool_config: ::std::option::Option<crate::types::ToolConfiguration>,
    /// <p>Configuration information for a guardrail that you want to use in the request.</p>
    pub guardrail_config: ::std::option::Option<crate::types::GuardrailStreamConfiguration>,
    /// <p>Additional inference parameters that the model supports, beyond the base set of inference parameters that <code>ConverseStream</code> supports in the <code>inferenceConfig</code> field.</p>
    pub additional_model_request_fields: ::std::option::Option<::aws_smithy_types::Document>,
    /// <p>Additional model parameters field paths to return in the response. <code>ConverseStream</code> returns the requested fields as a JSON Pointer object in the <code>additionalModelResponseFields</code> field. The following is example JSON for <code>additionalModelResponseFieldPaths</code>.</p>
    /// <p><code>\[ "/stop_sequence" \]</code></p>
    /// <p>For information about the JSON Pointer syntax, see the <a href="https://datatracker.ietf.org/doc/html/rfc6901">Internet Engineering Task Force (IETF)</a> documentation.</p>
    /// <p><code>ConverseStream</code> rejects an empty JSON Pointer or incorrectly structured JSON Pointer with a <code>400</code> error code. if the JSON Pointer is valid, but the requested field is not in the model response, it is ignored by <code>ConverseStream</code>.</p>
    pub additional_model_response_field_paths: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
}
impl ConverseStreamInput {
    /// <p>The ID for the model.</p>
    /// <p>The <code>modelId</code> to provide depends on the type of model that you use:</p>
    /// <ul>
    /// <li>
    /// <p>If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns">Amazon Bedrock base model IDs (on-demand throughput)</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html">Run inference using a Provisioned Throughput</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a custom model, first purchase Provisioned Throughput for it. Then specify the ARN of the resulting provisioned model. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html">Use a custom model in Amazon Bedrock</a> in the Amazon Bedrock User Guide.</p></li>
    /// </ul>
    pub fn model_id(&self) -> ::std::option::Option<&str> {
        self.model_id.as_deref()
    }
    /// <p>The messages that you want to send to the model.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.messages.is_none()`.
    pub fn messages(&self) -> &[crate::types::Message] {
        self.messages.as_deref().unwrap_or_default()
    }
    /// <p>A system prompt to send to the model.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.system.is_none()`.
    pub fn system(&self) -> &[crate::types::SystemContentBlock] {
        self.system.as_deref().unwrap_or_default()
    }
    /// <p>Inference parameters to pass to the model. <code>ConverseStream</code> supports a base set of inference parameters. If you need to pass additional parameters that the model supports, use the <code>additionalModelRequestFields</code> request field.</p>
    pub fn inference_config(&self) -> ::std::option::Option<&crate::types::InferenceConfiguration> {
        self.inference_config.as_ref()
    }
    /// <p>Configuration information for the tools that the model can use when generating a response.</p><note>
    /// <p>This field is only supported by Anthropic Claude 3 models.</p>
    /// </note>
    pub fn tool_config(&self) -> ::std::option::Option<&crate::types::ToolConfiguration> {
        self.tool_config.as_ref()
    }
    /// <p>Configuration information for a guardrail that you want to use in the request.</p>
    pub fn guardrail_config(&self) -> ::std::option::Option<&crate::types::GuardrailStreamConfiguration> {
        self.guardrail_config.as_ref()
    }
    /// <p>Additional inference parameters that the model supports, beyond the base set of inference parameters that <code>ConverseStream</code> supports in the <code>inferenceConfig</code> field.</p>
    pub fn additional_model_request_fields(&self) -> ::std::option::Option<&::aws_smithy_types::Document> {
        self.additional_model_request_fields.as_ref()
    }
    /// <p>Additional model parameters field paths to return in the response. <code>ConverseStream</code> returns the requested fields as a JSON Pointer object in the <code>additionalModelResponseFields</code> field. The following is example JSON for <code>additionalModelResponseFieldPaths</code>.</p>
    /// <p><code>\[ "/stop_sequence" \]</code></p>
    /// <p>For information about the JSON Pointer syntax, see the <a href="https://datatracker.ietf.org/doc/html/rfc6901">Internet Engineering Task Force (IETF)</a> documentation.</p>
    /// <p><code>ConverseStream</code> rejects an empty JSON Pointer or incorrectly structured JSON Pointer with a <code>400</code> error code. if the JSON Pointer is valid, but the requested field is not in the model response, it is ignored by <code>ConverseStream</code>.</p>
    ///
    /// If no value was sent for this field, a default will be set. If you want to determine if no value was sent, use `.additional_model_response_field_paths.is_none()`.
    pub fn additional_model_response_field_paths(&self) -> &[::std::string::String] {
        self.additional_model_response_field_paths.as_deref().unwrap_or_default()
    }
}
impl ConverseStreamInput {
    /// Creates a new builder-style object to manufacture [`ConverseStreamInput`](crate::operation::converse_stream::ConverseStreamInput).
    pub fn builder() -> crate::operation::converse_stream::builders::ConverseStreamInputBuilder {
        crate::operation::converse_stream::builders::ConverseStreamInputBuilder::default()
    }
}

/// A builder for [`ConverseStreamInput`](crate::operation::converse_stream::ConverseStreamInput).
#[derive(::std::clone::Clone, ::std::cmp::PartialEq, ::std::default::Default, ::std::fmt::Debug)]
#[non_exhaustive]
pub struct ConverseStreamInputBuilder {
    pub(crate) model_id: ::std::option::Option<::std::string::String>,
    pub(crate) messages: ::std::option::Option<::std::vec::Vec<crate::types::Message>>,
    pub(crate) system: ::std::option::Option<::std::vec::Vec<crate::types::SystemContentBlock>>,
    pub(crate) inference_config: ::std::option::Option<crate::types::InferenceConfiguration>,
    pub(crate) tool_config: ::std::option::Option<crate::types::ToolConfiguration>,
    pub(crate) guardrail_config: ::std::option::Option<crate::types::GuardrailStreamConfiguration>,
    pub(crate) additional_model_request_fields: ::std::option::Option<::aws_smithy_types::Document>,
    pub(crate) additional_model_response_field_paths: ::std::option::Option<::std::vec::Vec<::std::string::String>>,
}
impl ConverseStreamInputBuilder {
    /// <p>The ID for the model.</p>
    /// <p>The <code>modelId</code> to provide depends on the type of model that you use:</p>
    /// <ul>
    /// <li>
    /// <p>If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns">Amazon Bedrock base model IDs (on-demand throughput)</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html">Run inference using a Provisioned Throughput</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a custom model, first purchase Provisioned Throughput for it. Then specify the ARN of the resulting provisioned model. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html">Use a custom model in Amazon Bedrock</a> in the Amazon Bedrock User Guide.</p></li>
    /// </ul>
    /// This field is required.
    pub fn model_id(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        self.model_id = ::std::option::Option::Some(input.into());
        self
    }
    /// <p>The ID for the model.</p>
    /// <p>The <code>modelId</code> to provide depends on the type of model that you use:</p>
    /// <ul>
    /// <li>
    /// <p>If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns">Amazon Bedrock base model IDs (on-demand throughput)</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html">Run inference using a Provisioned Throughput</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a custom model, first purchase Provisioned Throughput for it. Then specify the ARN of the resulting provisioned model. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html">Use a custom model in Amazon Bedrock</a> in the Amazon Bedrock User Guide.</p></li>
    /// </ul>
    pub fn set_model_id(mut self, input: ::std::option::Option<::std::string::String>) -> Self {
        self.model_id = input;
        self
    }
    /// <p>The ID for the model.</p>
    /// <p>The <code>modelId</code> to provide depends on the type of model that you use:</p>
    /// <ul>
    /// <li>
    /// <p>If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns">Amazon Bedrock base model IDs (on-demand throughput)</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html">Run inference using a Provisioned Throughput</a> in the Amazon Bedrock User Guide.</p></li>
    /// <li>
    /// <p>If you use a custom model, first purchase Provisioned Throughput for it. Then specify the ARN of the resulting provisioned model. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html">Use a custom model in Amazon Bedrock</a> in the Amazon Bedrock User Guide.</p></li>
    /// </ul>
    pub fn get_model_id(&self) -> &::std::option::Option<::std::string::String> {
        &self.model_id
    }
    /// Appends an item to `messages`.
    ///
    /// To override the contents of this collection use [`set_messages`](Self::set_messages).
    ///
    /// <p>The messages that you want to send to the model.</p>
    pub fn messages(mut self, input: crate::types::Message) -> Self {
        let mut v = self.messages.unwrap_or_default();
        v.push(input);
        self.messages = ::std::option::Option::Some(v);
        self
    }
    /// <p>The messages that you want to send to the model.</p>
    pub fn set_messages(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::Message>>) -> Self {
        self.messages = input;
        self
    }
    /// <p>The messages that you want to send to the model.</p>
    pub fn get_messages(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::Message>> {
        &self.messages
    }
    /// Appends an item to `system`.
    ///
    /// To override the contents of this collection use [`set_system`](Self::set_system).
    ///
    /// <p>A system prompt to send to the model.</p>
    pub fn system(mut self, input: crate::types::SystemContentBlock) -> Self {
        let mut v = self.system.unwrap_or_default();
        v.push(input);
        self.system = ::std::option::Option::Some(v);
        self
    }
    /// <p>A system prompt to send to the model.</p>
    pub fn set_system(mut self, input: ::std::option::Option<::std::vec::Vec<crate::types::SystemContentBlock>>) -> Self {
        self.system = input;
        self
    }
    /// <p>A system prompt to send to the model.</p>
    pub fn get_system(&self) -> &::std::option::Option<::std::vec::Vec<crate::types::SystemContentBlock>> {
        &self.system
    }
    /// <p>Inference parameters to pass to the model. <code>ConverseStream</code> supports a base set of inference parameters. If you need to pass additional parameters that the model supports, use the <code>additionalModelRequestFields</code> request field.</p>
    pub fn inference_config(mut self, input: crate::types::InferenceConfiguration) -> Self {
        self.inference_config = ::std::option::Option::Some(input);
        self
    }
    /// <p>Inference parameters to pass to the model. <code>ConverseStream</code> supports a base set of inference parameters. If you need to pass additional parameters that the model supports, use the <code>additionalModelRequestFields</code> request field.</p>
    pub fn set_inference_config(mut self, input: ::std::option::Option<crate::types::InferenceConfiguration>) -> Self {
        self.inference_config = input;
        self
    }
    /// <p>Inference parameters to pass to the model. <code>ConverseStream</code> supports a base set of inference parameters. If you need to pass additional parameters that the model supports, use the <code>additionalModelRequestFields</code> request field.</p>
    pub fn get_inference_config(&self) -> &::std::option::Option<crate::types::InferenceConfiguration> {
        &self.inference_config
    }
    /// <p>Configuration information for the tools that the model can use when generating a response.</p><note>
    /// <p>This field is only supported by Anthropic Claude 3 models.</p>
    /// </note>
    pub fn tool_config(mut self, input: crate::types::ToolConfiguration) -> Self {
        self.tool_config = ::std::option::Option::Some(input);
        self
    }
    /// <p>Configuration information for the tools that the model can use when generating a response.</p><note>
    /// <p>This field is only supported by Anthropic Claude 3 models.</p>
    /// </note>
    pub fn set_tool_config(mut self, input: ::std::option::Option<crate::types::ToolConfiguration>) -> Self {
        self.tool_config = input;
        self
    }
    /// <p>Configuration information for the tools that the model can use when generating a response.</p><note>
    /// <p>This field is only supported by Anthropic Claude 3 models.</p>
    /// </note>
    pub fn get_tool_config(&self) -> &::std::option::Option<crate::types::ToolConfiguration> {
        &self.tool_config
    }
    /// <p>Configuration information for a guardrail that you want to use in the request.</p>
    pub fn guardrail_config(mut self, input: crate::types::GuardrailStreamConfiguration) -> Self {
        self.guardrail_config = ::std::option::Option::Some(input);
        self
    }
    /// <p>Configuration information for a guardrail that you want to use in the request.</p>
    pub fn set_guardrail_config(mut self, input: ::std::option::Option<crate::types::GuardrailStreamConfiguration>) -> Self {
        self.guardrail_config = input;
        self
    }
    /// <p>Configuration information for a guardrail that you want to use in the request.</p>
    pub fn get_guardrail_config(&self) -> &::std::option::Option<crate::types::GuardrailStreamConfiguration> {
        &self.guardrail_config
    }
    /// <p>Additional inference parameters that the model supports, beyond the base set of inference parameters that <code>ConverseStream</code> supports in the <code>inferenceConfig</code> field.</p>
    pub fn additional_model_request_fields(mut self, input: ::aws_smithy_types::Document) -> Self {
        self.additional_model_request_fields = ::std::option::Option::Some(input);
        self
    }
    /// <p>Additional inference parameters that the model supports, beyond the base set of inference parameters that <code>ConverseStream</code> supports in the <code>inferenceConfig</code> field.</p>
    pub fn set_additional_model_request_fields(mut self, input: ::std::option::Option<::aws_smithy_types::Document>) -> Self {
        self.additional_model_request_fields = input;
        self
    }
    /// <p>Additional inference parameters that the model supports, beyond the base set of inference parameters that <code>ConverseStream</code> supports in the <code>inferenceConfig</code> field.</p>
    pub fn get_additional_model_request_fields(&self) -> &::std::option::Option<::aws_smithy_types::Document> {
        &self.additional_model_request_fields
    }
    /// Appends an item to `additional_model_response_field_paths`.
    ///
    /// To override the contents of this collection use [`set_additional_model_response_field_paths`](Self::set_additional_model_response_field_paths).
    ///
    /// <p>Additional model parameters field paths to return in the response. <code>ConverseStream</code> returns the requested fields as a JSON Pointer object in the <code>additionalModelResponseFields</code> field. The following is example JSON for <code>additionalModelResponseFieldPaths</code>.</p>
    /// <p><code>\[ "/stop_sequence" \]</code></p>
    /// <p>For information about the JSON Pointer syntax, see the <a href="https://datatracker.ietf.org/doc/html/rfc6901">Internet Engineering Task Force (IETF)</a> documentation.</p>
    /// <p><code>ConverseStream</code> rejects an empty JSON Pointer or incorrectly structured JSON Pointer with a <code>400</code> error code. if the JSON Pointer is valid, but the requested field is not in the model response, it is ignored by <code>ConverseStream</code>.</p>
    pub fn additional_model_response_field_paths(mut self, input: impl ::std::convert::Into<::std::string::String>) -> Self {
        let mut v = self.additional_model_response_field_paths.unwrap_or_default();
        v.push(input.into());
        self.additional_model_response_field_paths = ::std::option::Option::Some(v);
        self
    }
    /// <p>Additional model parameters field paths to return in the response. <code>ConverseStream</code> returns the requested fields as a JSON Pointer object in the <code>additionalModelResponseFields</code> field. The following is example JSON for <code>additionalModelResponseFieldPaths</code>.</p>
    /// <p><code>\[ "/stop_sequence" \]</code></p>
    /// <p>For information about the JSON Pointer syntax, see the <a href="https://datatracker.ietf.org/doc/html/rfc6901">Internet Engineering Task Force (IETF)</a> documentation.</p>
    /// <p><code>ConverseStream</code> rejects an empty JSON Pointer or incorrectly structured JSON Pointer with a <code>400</code> error code. if the JSON Pointer is valid, but the requested field is not in the model response, it is ignored by <code>ConverseStream</code>.</p>
    pub fn set_additional_model_response_field_paths(mut self, input: ::std::option::Option<::std::vec::Vec<::std::string::String>>) -> Self {
        self.additional_model_response_field_paths = input;
        self
    }
    /// <p>Additional model parameters field paths to return in the response. <code>ConverseStream</code> returns the requested fields as a JSON Pointer object in the <code>additionalModelResponseFields</code> field. The following is example JSON for <code>additionalModelResponseFieldPaths</code>.</p>
    /// <p><code>\[ "/stop_sequence" \]</code></p>
    /// <p>For information about the JSON Pointer syntax, see the <a href="https://datatracker.ietf.org/doc/html/rfc6901">Internet Engineering Task Force (IETF)</a> documentation.</p>
    /// <p><code>ConverseStream</code> rejects an empty JSON Pointer or incorrectly structured JSON Pointer with a <code>400</code> error code. if the JSON Pointer is valid, but the requested field is not in the model response, it is ignored by <code>ConverseStream</code>.</p>
    pub fn get_additional_model_response_field_paths(&self) -> &::std::option::Option<::std::vec::Vec<::std::string::String>> {
        &self.additional_model_response_field_paths
    }
    /// Consumes the builder and constructs a [`ConverseStreamInput`](crate::operation::converse_stream::ConverseStreamInput).
    pub fn build(
        self,
    ) -> ::std::result::Result<crate::operation::converse_stream::ConverseStreamInput, ::aws_smithy_types::error::operation::BuildError> {
        ::std::result::Result::Ok(crate::operation::converse_stream::ConverseStreamInput {
            model_id: self.model_id,
            messages: self.messages,
            system: self.system,
            inference_config: self.inference_config,
            tool_config: self.tool_config,
            guardrail_config: self.guardrail_config,
            additional_model_request_fields: self.additional_model_request_fields,
            additional_model_response_field_paths: self.additional_model_response_field_paths,
        })
    }
}
