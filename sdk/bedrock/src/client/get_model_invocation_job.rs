// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
impl super::Client {
    /// Constructs a fluent builder for the [`GetModelInvocationJob`](crate::operation::get_model_invocation_job::builders::GetModelInvocationJobFluentBuilder) operation.
    ///
    /// - The fluent builder is configurable:
    ///   - [`job_identifier(impl Into<String>)`](crate::operation::get_model_invocation_job::builders::GetModelInvocationJobFluentBuilder::job_identifier) / [`set_job_identifier(Option<String>)`](crate::operation::get_model_invocation_job::builders::GetModelInvocationJobFluentBuilder::set_job_identifier):<br>required: **true**<br><p>The Amazon Resource Name (ARN) of the batch inference job.</p><br>
    /// - On success, responds with [`GetModelInvocationJobOutput`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput) with field(s):
    ///   - [`job_arn(String)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::job_arn): <p>The Amazon Resource Name (ARN) of the batch inference job.</p>
    ///   - [`job_name(Option<String>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::job_name): <p>The name of the batch inference job.</p>
    ///   - [`model_id(String)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::model_id): <p>The unique identifier of the foundation model used for model inference.</p>
    ///   - [`client_request_token(Option<String>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::client_request_token): <p>A unique, case-sensitive identifier to ensure that the API request completes no more than one time. If this token matches a previous request, Amazon Bedrock ignores the request, but does not return an error. For more information, see <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Run_Instance_Idempotency.html">Ensuring idempotency</a>.</p>
    ///   - [`role_arn(String)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::role_arn): <p>The Amazon Resource Name (ARN) of the service role with permissions to carry out and manage batch inference. You can use the console to create a default service role or follow the steps at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-iam-sr.html">Create a service role for batch inference</a>.</p>
    ///   - [`status(Option<ModelInvocationJobStatus>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::status): <p>The status of the batch inference job.</p> <p>The following statuses are possible:</p> <ul>  <li>   <p>Submitted – This job has been submitted to a queue for validation.</p></li>  <li>   <p>Validating – This job is being validated for the requirements described in <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-data.html">Format and upload your batch inference data</a>. The criteria include the following:</p>   <ul>    <li>     <p>Your IAM service role has access to the Amazon S3 buckets containing your files.</p></li>    <li>     <p>Your files are .jsonl files and each individual record is a JSON object in the correct format. Note that validation doesn't check if the <code>modelInput</code> value matches the request body for the model.</p></li>    <li>     <p>Your files fulfill the requirements for file size and number of records. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html">Quotas for Amazon Bedrock</a>.</p></li>   </ul></li>  <li>   <p>Scheduled – This job has been validated and is now in a queue. The job will automatically start when it reaches its turn.</p></li>  <li>   <p>Expired – This job timed out because it was scheduled but didn't begin before the set timeout duration. Submit a new job request.</p></li>  <li>   <p>InProgress – This job has begun. You can start viewing the results in the output S3 location.</p></li>  <li>   <p>Completed – This job has successfully completed. View the output files in the output S3 location.</p></li>  <li>   <p>PartiallyCompleted – This job has partially completed. Not all of your records could be processed in time. View the output files in the output S3 location.</p></li>  <li>   <p>Failed – This job has failed. Check the failure message for any further details. For further assistance, reach out to the <a href="https://console.aws.amazon.com/support/home/">Amazon Web ServicesSupport Center</a>.</p></li>  <li>   <p>Stopped – This job was stopped by a user.</p></li>  <li>   <p>Stopping – This job is being stopped by a user.</p></li> </ul>
    ///   - [`message(Option<String>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::message): <p>If the batch inference job failed, this field contains a message describing why the job failed.</p>
    ///   - [`submit_time(DateTime)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::submit_time): <p>The time at which the batch inference job was submitted.</p>
    ///   - [`last_modified_time(Option<DateTime>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::last_modified_time): <p>The time at which the batch inference job was last modified.</p>
    ///   - [`end_time(Option<DateTime>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::end_time): <p>The time at which the batch inference job ended.</p>
    ///   - [`input_data_config(Option<ModelInvocationJobInputDataConfig>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::input_data_config): <p>Details about the location of the input to the batch inference job.</p>
    ///   - [`output_data_config(Option<ModelInvocationJobOutputDataConfig>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::output_data_config): <p>Details about the location of the output of the batch inference job.</p>
    ///   - [`vpc_config(Option<VpcConfig>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::vpc_config): <p>The configuration of the Virtual Private Cloud (VPC) for the data in the batch inference job. For more information, see <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/batch-vpc">Protect batch inference jobs using a VPC</a>.</p>
    ///   - [`timeout_duration_in_hours(Option<i32>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::timeout_duration_in_hours): <p>The number of hours after which batch inference job was set to time out.</p>
    ///   - [`job_expiration_time(Option<DateTime>)`](crate::operation::get_model_invocation_job::GetModelInvocationJobOutput::job_expiration_time): <p>The time at which the batch inference job times or timed out.</p>
    /// - On failure, responds with [`SdkError<GetModelInvocationJobError>`](crate::operation::get_model_invocation_job::GetModelInvocationJobError)
    pub fn get_model_invocation_job(&self) -> crate::operation::get_model_invocation_job::builders::GetModelInvocationJobFluentBuilder {
        crate::operation::get_model_invocation_job::builders::GetModelInvocationJobFluentBuilder::new(self.handle.clone())
    }
}
